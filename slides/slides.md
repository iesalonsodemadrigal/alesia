# Slide 1: Presentación
---

# Slide 2: Problema (Ian)

¿Alguna vez os habéis puesto en el lugar de un compañero con discapacidad auditiva?

Imaginad situaciones tan cotidianas en clase como hablar con un profesor o pedir ayuda a un compañero.

Sabemos que en el aula puede haber un intérprete...

Pero, ¿qué pasa entre clase y clase? ¿y en el recreo? ¿y cuando quedamos para hacer un trabajo en grupo?

Son momentos clave para conocernos mejor.

Y ahí, la comunicación se rompe.

# Slide 3: Alesia (Ian)

¿Puede la innovación tecnológica actual, como la Inteligencia Artificial, ofrecer nuevas soluciones a este problema?
Esa es la pregunta que nos impulsó a iniciar esta investigación.

Y así nació ALESIA:
Nuestro proyecto de investigación aplicada.

Una aplicación que interpreta conversaciones a Lengua de Signos Española mediante un avatar 3D.

# Slide 4: Estructura app (Ian)

Aquí podéis ver a Alesia. Es multiplataforma: se puede usar en un móvil, un reloj e incluso en unas gafas inteligentes.

Al otro lado representamos visualmente los pasos que realiza internamente.

Vamos ahora a explicar las cuatro fases que forman el desarrollo experimental.

# Slide 5 (David)

El primer paso es transcribir el mensaje de voz a texto.

Probamos distintas herramientas de transcripción y elegimos la más rápida y precisa para conseguir una conversación
fluida.

En el primer cuadro de la aplicación simulamos la transcripción de un mensaje.

# Slide 6 (David)

Ya tenemos el mensaje en texto… pero no basta con traducir palabra por palabra. La Lengua de Signos tiene sus propias
reglas gramaticales.

Para resolver esto, entrenamos un modelo de Inteligencia Artificial capaz de transformar la frase original al formato
correcto para que pueda ser entendida en lengua de signos.

En el segundo cuadro de texto podéis ver cómo queda el mensaje tras aplicar la gramática de lengua de signos.

# Slide 7 (Ales)

Aquí empieza la magia. Esta es probablemente la parte más compleja del proyecto: transformar los movimientos humanos en
datos que la máquina pueda entender.

# Slide 8 (Ales)

Hemos desarrollado un algoritmo capaz de analizar vídeos y extraer las coordenadas en 3D de cada parte del cuerpo.
Con esta información, creamos una base de datos que nos permite buscar una palabra y obtener las coordenadas en tiempo
real.

# Slide 9 (David)

Por último, obtenidas las coordenadas, pasamos esta información a Alesia.

El resultado es una representación visual del mensaje en Lengua de Signos.

# Slide 10 (David)

El camino hasta aquí no ha sido fácil. Hemos pasado muchas horas conociendo la gramática de la lengua de signos,
analizando vídeos o desarrollando algoritmos para transformar coordenadas.

Un vídeo de apenas 10 segundos pero que a nosotros nos ha llevado un curso.

# Slide 11 (David)

Aquí podéis ver la precisión y el realismo de Alesia.

# Slide 12 (Ian)

A través de este cuestionario recogemos la valoración de los intérpretes.

Estamos muy orgullosos de los resultados. La valoración media ha sido positiva.

Como mejora destacada, señalan la falta de expresión facial, algo fundamental en lengua de signos.

Es una mejora viable, ya que la falta de expresión se debe a que los vídeos analizados carecían de gestos faciales.

# Slide 13 (Ales)

Como conclusiones destacadas:

- Creemos viable el desarrollo de un producto final.
- Hemos visto que la expresión facial es una parte clave.
- La Inteligencia Artificial ha sido fundamental en el proyecto.
- Y lo más importante: Alesia no es solo una herramienta tecnológica. Es una herramienta para la igualdad de
  oportunidades.

# Slide 14 (Ales)

Para acabar, nos hace ilusión ver que en febrero de 2025 NVIDIA presentó un proyecto muy similar.

Nosotros empezamos en octubre de 2024, con menos recursos pero la misma visión.

Eso nos confirma que vamos por buen camino y nos anima a seguir investigando.

Hoy como alumnos, mañana como desarrolladores, creemos que la tecnología debe estar al servicio de la inclusión.

# Slide 15:

Muchas gracias.

# Otras ideas:

- Usamos unity por ser menos costoso que la generación de vídeos con IA.