# Slide 1: Presentación
---

# Slide 2: Problema

Te despiertas, desayunas y vas a clase...

Intentas comunicarte con tu profesor, realizar actividades en equipo, pedir ayuda a un compañero... pero nadie te
entiende.

Esta es la realidad diaria de miles de personas con discapacidad auditiva que utilizan la Lengua de Signos como medio de
comunicación.

Personas que se enfrentan a barreras de comunicación constantes que limitan su acceso a la educación, la salud o el
empleo.

# Slide 3: Alesia

Frente al silencio, nace la innovación.

Alesia es nuestro proyecto de investigación aplicada: una aplicación capaz de escuchar una conversación, entenderla y
traducirla automáticamente a Lengua de Signos Española.

# Slide 4: Estructura app

El proyecto es multiplataforma, se puede usar en un móvil, un reloj e incluso en unas gafas inteligentes.

Utilizamos Inteligencia Artificial para convertir voz en texto, reorganizar el mensaje con la gramática de la lengua de
signos y representar la interpretación en un avatar 3D llamada Alesia.

Vamos ahora a explicar las cuatro fases que forman el desarrollo experimental.

# Slide 5:

Para que Alesia pueda interpretar, lo primero es escuchar y entender lo que se dice.

Analizamos diferentes herramientas de transcripción automática y seleccionamos la más adecuada, equilibrando velocidad y
precisión, para asegurar una experiencia fluida y en tiempo real.

# Slide 6:

Ya tenemos el mensaje en texto… pero no basta con traducir palabra por palabra.

La Lengua de Signos no es una traducción literal del español. Tiene su propio orden, sus propias reglas. Por ejemplo, no
se dice 'Hola, ¿cómo estáis hoy? espero que te encuentres bien.', sino algo como 'Hola. Tu como hoy yo esperar tu
bien.'.

Para resolver esto, entrenamos un modelo de Inteligencia Artificial capaz de transformar la frase original al formato
correcto para que pueda ser entendida en lengua de signos.

# Slide 7:

Aquí empieza la magia. Esta es probablemente la parte más compleja del proyecto: transformar (codificar) los
movimientos humanos en datos que la máquina pueda entender.

Hemos desarrollado un script capaz de analizar vídeos del diccionario de lengua de signos y extraer las coordenadas en 3D
de cada parte del cuerpo. -- implicada en la interpretación.--

Con esta información, creamos una base de datos que nos permite buscar una palabra y obtener su traducción en
coordenadas. ----Es como si le estuviéramos enseñando al avatar el abecedario de su cuerpo----.

# Slide 8:

# Slide 9:

# Slide 10:

- La validación de estas fases representan la parte cuantitativa de nuestro enfoque mixto de investigación.

# Slide 11:

# Otras ideas:

- Usamos unity por ser menos costoso que la generación de vídeos con IA.