# Slide 1: Presentación
---

# Slide 2: Problema (Ian)

¿Alguna vez os habéis puesto en el lugar de un compañero con discapacidad auditiva?

Imaginad situaciones tan cotidianas en clase como hablar con un profesor o pedir ayuda a un compañero.

Sabemos que en el aula puede haber un intérprete...

Pero, ¿qué pasa entre clase y clase? ¿y en el recreo? ¿y cuando quedamos para hacer un trabajo en grupo?

Son momentos clave para conocernos mejor.

Y ahí, la comunicación se rompe.

# Slide 3: Alesia (Ian)

¿Puede la innovación tecnológica actual, como la Inteligencia Artificial, ofrecer nuevas soluciones a este problema?
Esa es la pregunta que nos impulsó a iniciar esta investigación.

Y así nació ALESIA:
Nuestro proyecto de investigación aplicada.  

Una aplicación que interpreta conversaciones a Lengua de Signos Española mediante un avatar 3D.

# Slide 4: Estructura app (Ian)

Aquí podéis ver a Alesia. Es multiplataforma: se puede usar en un móvil, un reloj e incluso en unas gafas inteligentes.

Al otro lado representamos visualmente los pasos que realiza internamente.

Utilizamos Inteligencia Artificial para convertir voz en texto, aplicar la gramática de la lengua de
signos e interpretarlos.

Vamos ahora a explicar las cuatro fases que forman el desarrollo experimental.

# Slide 5 (David)

Para que Alesia pueda interpretar, lo primero es escuchar y entender lo que se dice.

Analizamos diferentes herramientas de transcripción automática y seleccionamos la más adecuada, equilibrando velocidad y
precisión, para asegurar una experiencia fluida y en tiempo real.

En el primer cuadro de la aplicación podéis ver el texto del mensaje de voz.

# Slide 6 (David)

Ya tenemos el mensaje en texto… pero no basta con traducir palabra por palabra. La Lengua de Signos tiene sus propias
reglas gramaticales.

Para resolver esto, entrenamos un modelo de Inteligencia Artificial capaz de transformar la frase original al formato
correcto para que pueda ser entendida en lengua de signos.

En el segundo cuadro de texto podéis ver cómo queda el mensaje tras aplicar la gramática de lengua de signos.

# Slide 7 (Ales)

Aquí empieza la magia. Esta es probablemente la parte más compleja del proyecto: transformar los movimientos humanos en
datos que la máquina pueda entender.

Hemos desarrollado un algoritmo capaz de analizar vídeos y extraer las coordenadas en 3D de cada parte del cuerpo.

Con esta información, creamos una base de datos que nos permite buscar una palabra y obtener las coordenadas en tiempo
real.

# Slide 8 (Ales)

-------- Relacionada con Slide 7: Se ve un vídeo con las coordenadas

# Slide 9 (David)

Por último, obtenidas las coordenadas, pasamos esta información a Alesia.

El resultado es una representación visual del mensaje en Lengua de Signos.

La validación de estas fases representan la parte cuantitativa de nuestro enfoque mixto de investigación.

# Slide 10 (David)

El camino hasta aquí no ha sido fácil. Hemos pasado muchas horas estudiando la lengua de signos, analizando vídeos,
desarrollando algoritmos para transformar coordenadas.

Un vídeo de apenas 10 segundos pero que a nosotros nos ha llevado un curso.

# Slide 11 (David)

Aquí podéis ver la precisión y el realismo de Alesia.

# Slide 12 (Ian)

Para el enfoque cualitativo de nuestra investigación, diseñamos un cuestionario propio.

Estamos muy orgullosos de los resultados. La valoración media ha sido positiva.

Como mejora destacada, señalan la falta de expresión facial, algo fundamental en lengua de signos.

Es una mejora viable, ya que la falta de expresión se debe a que los vídeos analizados carecían de gestos faciales.

# Slide 13 (Ales)

Como conclusiones destacadas:

- Es posible desarrollar una aplicación capaz de traducir la lengua hablada a la Lengua de Signos en tiempo real.
- Hemos visto que la expresión facial es una parte clave que aún debemos incorporar.
- La Inteligencia Artificial ha sido fundamental en cada fase del proyecto: sin ella, esto no sería posible.
- Y lo más importante: Alesia no es solo una herramienta tecnológica. Es una herramienta para la igualdad de
  oportunidades.

# Slide 14 (Ales)

Para acabar, nos hace mucha ilusión ver que en febrero de 2025 NVIDIA presentó un proyecto muy similar al nuestro.

Empezamos en octubre de 2024, con menos recursos pero la misma visión.

Eso nos confirma que vamos por buen camino y nos anima a seguir investigando.

Hoy como alumnos, mañana como desarrolladores, creemos que la tecnología debe estar al servicio de la inclusión.

# Slide 15:

Muchas gracias (representación signo 'gracias')

# Otras ideas:

- Usamos unity por ser menos costoso que la generación de vídeos con IA.